# PyTorch CUDA Kurulum KÄ±lavuzu - RTX 4060

## ğŸ¯ Sistem Gereksinimlerini Kontrol Edin

### 1. NVIDIA SÃ¼rÃ¼cÃ¼ ve CUDA KontrolÃ¼

Terminalinizde ÅŸunu Ã§alÄ±ÅŸtÄ±rÄ±n:

```powershell
nvidia-smi
```

**Beklenen Ã§Ä±ktÄ±:**
```
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 581.08                 Driver Version: 581.08         CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0  On |                  N/A |
```

âœ… **Ã–nemli:** `CUDA Version` gÃ¶rÃ¼nÃ¼yor mu? Evet ise devam edebilirsiniz.

---

## ğŸ“¦ PyTorch CUDA Kurulumu

### AdÄ±m 1: Mevcut PyTorch'u KaldÄ±rÄ±n (Varsa)

```powershell
pip uninstall -y torch torchvision torchaudio
```

### AdÄ±m 2: CUDA 12.4 ile PyTorch Kurulumu (Ã–NERÄ°LÄ°R)

**RTX 4060 iÃ§in en iyi seÃ§enek: CUDA 12.4**

```powershell
pip install --index-url https://download.pytorch.org/whl/cu124 torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1
```

### Alternatif: CUDA 12.1 ile PyTorch

CUDA 12.4'te sorun yaÅŸarsanÄ±z:

```powershell
pip install --index-url https://download.pytorch.org/whl/cu121 torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1
```

### AdÄ±m 3: Kurulumu DoÄŸrulayÄ±n

```powershell
python -c "import torch; print('PyTorch version:', torch.__version__); print('CUDA available:', torch.cuda.is_available()); print('CUDA version:', torch.version.cuda); print('Device name:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A')"
```

**Beklenen Ã§Ä±ktÄ±:**
```
PyTorch version: 2.5.1+cu124
CUDA available: True
CUDA version: 12.4
Device name: NVIDIA GeForce RTX 4060 Laptop GPU
```

âœ… **CUDA available: True** gÃ¶rmeli ve GPU adÄ±nÄ±z doÄŸru olmalÄ±!

---

## ğŸ”§ DetaylÄ± Test (Opsiyonel)

Sigma Analyst kurulumu yaptÄ±ysanÄ±z, detaylÄ± GPU testi Ã§alÄ±ÅŸtÄ±rÄ±n:

```powershell
python -m backend.config.compute_config
```

**Beklenen Ã§Ä±ktÄ±:**
```
======================================================================
ğŸ”§ GPU Setup Test - Sigma Analyst
======================================================================

1ï¸âƒ£  PyTorch Installation:
   PyTorch Version: 2.5.1+cu124
   CUDA Available: True
   CUDA Version: 12.4
   cuDNN Version: 90100
   Device Count: 1
   Device Name: NVIDIA GeForce RTX 4060 Laptop GPU
   VRAM: 8.0 GB
   Compute Capability: 8.9
   BF16 Support: âœ… Yes
   TF32 Support: âœ… Yes

2ï¸âƒ£  Compute Manager Test:
============================================================
ğŸ–¥ï¸  Compute Mode: HYBRID
============================================================

ğŸ® GPU Information:
   Name: NVIDIA GeForce RTX 4060 Laptop GPU
   VRAM: 8.0 GB
   Compute Capability: 8.9
   CUDA Version: 12.4
   cuDNN Version: 90100

ğŸ“Š Workload Distribution:
   â€¢ Deep Learning (LSTM/Transformer): CUDA
   â€¢ ML Tree Models (XGBoost/CatBoost): CPU
   â€¢ RL Training (PPO): CUDA
   â€¢ Technical Analysis: CPU
   â€¢ Backtest Engine: CPU

âš™ï¸  GPU Settings:
   â€¢ Batch Size: 128
   â€¢ Mixed Precision: torch.bfloat16
   â€¢ TF32 Enabled: True
   â€¢ BF16 Support: âœ… (Better stability than FP16)

âœ… TF32 enabled for matrix operations
âœ… Float32 matmul precision set to 'high'
âœ… cuDNN benchmark enabled

3ï¸âƒ£  Memory Test:
   Test allocation: 0.00 GB
   Free VRAM: 8.00 GB
   âœ… Memory test passed

4ï¸âƒ£  BF16/FP16 Test:
   Using: torch.bfloat16
   âœ… Mixed precision test passed

======================================================================
âœ… Setup test complete!
======================================================================
```

---

## âŒ Sorun Giderme

### Problem 1: "torch.cuda.is_available() = False"

**Ã‡Ã¶zÃ¼m 1: CUDA versiyonunu kontrol edin**

```powershell
# nvidia-smi Ã§Ä±ktÄ±sÄ±ndaki CUDA Version'a bakÄ±n
nvidia-smi

# CUDA 12.x ise:
pip install --index-url https://download.pytorch.org/whl/cu124 torch torchvision torchaudio

# CUDA 11.x ise (eski sÃ¼rÃ¼cÃ¼):
pip install --index-url https://download.pytorch.org/whl/cu118 torch torchvision torchaudio
```

**Ã‡Ã¶zÃ¼m 2: NVIDIA sÃ¼rÃ¼cÃ¼sÃ¼nÃ¼ gÃ¼ncelleyin**

https://www.nvidia.com/Download/index.aspx adresinden en gÃ¼ncel sÃ¼rÃ¼cÃ¼yÃ¼ indirin.

### Problem 2: "RuntimeError: CUDA out of memory"

**Ã‡Ã¶zÃ¼m: Batch size'Ä± azaltÄ±n**

```python
from backend.config.compute_config import get_compute

compute = get_compute()
batch_size = compute.suggest_batch_size(vram_usage_multiplier=0.5)  # Half VRAM
print(f"Reduced batch size: {batch_size}")
```

### Problem 3: "Could not load dynamic library 'cudnn64_8.dll'"

**Ã‡Ã¶zÃ¼m: cuDNN otomatik yÃ¼klenir**

PyTorch 2.5.1 cuDNN'i iÃ§erir. AyrÄ± kurulum gerekmez. Sorun devam ederse:

```powershell
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

---

## ğŸ“Š PyTorch CUDA VersiyonlarÄ±

### Hangi CUDA Versiyonunu SeÃ§meliyim?

| GPU | Ã–nerilen CUDA | Kurulum Komutu |
|-----|---------------|----------------|
| RTX 40xx (4090, 4080, 4070, **4060**) | **CUDA 12.4** | `--index-url https://download.pytorch.org/whl/cu124` |
| RTX 30xx (3090, 3080, 3070, 3060) | CUDA 12.1 | `--index-url https://download.pytorch.org/whl/cu121` |
| RTX 20xx (2080, 2070, 2060) | CUDA 11.8 | `--index-url https://download.pytorch.org/whl/cu118` |
| GTX 16xx (1660, 1650) | CUDA 11.8 | `--index-url https://download.pytorch.org/whl/cu118` |

**Notlar:**
- RTX 40xx iÃ§in CUDA 12.4 **en iyi performansÄ±** saÄŸlar
- CUDA 12.1 de Ã§alÄ±ÅŸÄ±r ama biraz daha yavaÅŸ olabilir
- Daha eski CUDA versiyonlarÄ± (11.x) RTX 40xx'te **BF16 ve TF32 desteÄŸi** vermez

---

## âœ… Kurulum TamamlandÄ±!

PyTorch CUDA kurulumu tamamlandÄ±ysa, geri kalan paketleri kurun:

```powershell
# requirements.txt'teki diÄŸer paketler
pip install -r requirements.txt

# TA-Lib (opsiyonel, Windows iÃ§in binary wheel)
# https://github.com/cgohlke/talib-build/releases
pip install ta_lib-0.6.8-cp312-cp312-win_amd64.whl
```

**Test edin:**

```powershell
python -m backend.config.compute_config
```

---

## ğŸš€ Performans Beklentileri (RTX 4060 8GB)

CUDA 12.4 + BF16 + TF32 ile:

| Model | CPU (12 cores) | GPU (RTX 4060) | Speedup |
|-------|----------------|----------------|---------|
| LSTM (100 epochs) | ~2000s (33 dk) | ~80s (1.3 dk) | **25x** |
| Transformer (100 epochs) | ~5000s (83 dk) | ~100s (1.7 dk) | **50x** |
| RL Training (100k steps) | ~1200s (20 dk) | ~120s (2 dk) | **10x** |
| XGBoost (10k samples) | ~5s | ~5s | 1x (CPU zaten hÄ±zlÄ±) |
| Technical Analysis | ~0.5s | N/A (CPU'da Ã§alÄ±ÅŸÄ±r) | - |
| Backtest (10k bars) | ~0.1s | N/A (CPU'da Ã§alÄ±ÅŸÄ±r) | - |

**Total Pipeline:** 8200s (2.3 saat) â†’ 480s (8 dakika) = **17x speedup**

---

## ğŸ“š Ek Kaynaklar

- [PyTorch Resmi DokÃ¼mantasyon](https://pytorch.org/get-started/locally/)
- [NVIDIA CUDA Downloads](https://developer.nvidia.com/cuda-downloads)
- [RTX 4060 Specs](https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4060-4060ti/)
- [Sigma Analyst GPU Usage Guide](./docs/GPU_USAGE_GUIDE.md)

---

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Her zaman CUDA 12.4 kullanÄ±n** (RTX 4060 iÃ§in)
2. **BF16 otomatik aktif** olacak (FP16'dan daha stabil)
3. **TF32 otomatik aktif** olacak (+%10-30 hÄ±z)
4. **Batch size otomatik ayarlanacak** (VRAM'e gÃ¶re)
5. **Tree modeller CPU'da kalacak** (kÃ¼Ã§Ã¼k data iÃ§in daha hÄ±zlÄ±)

---

Kurulumda sorun yaÅŸarsanÄ±z:
- GitHub Issues: https://github.com/your-repo/issues
- GPU test: `python -m backend.config.compute_config`
- VRAM monitoring: `nvidia-smi -l 1` (1 saniye refresh)
