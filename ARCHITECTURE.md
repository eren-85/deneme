# ğŸ—ï¸ SIGMA ANALYST - Sistem Mimarisi ve Teknik Ã–neriler

## ğŸ“Š PROJE GENEL BAKIÅ

**Sigma Analyst**: AI-powered Kripto Piyasa Analiz ve Ä°stihbarat Sistemi

### Temel Ã–zellikler
- ğŸ¤– Multi-role AI Agent (5 farklÄ± uzmanlÄ±k rolÃ¼)
- ğŸ“ˆ GerÃ§ek zamanlÄ± teknik, on-chain ve tÃ¼rev piyasa analizi
- ğŸ§  Reinforcement Learning ile sÃ¼rekli Ã¶ÄŸrenme
- ğŸ“± Modern React-based dashboard
- âš¡ Real-time monitoring ve alerting
- ğŸ¯ Trade fÄ±rsatÄ± tespiti ve risk yÃ¶netimi

---

## ğŸ¯ 1. YANIT: "LSTM, XGBoost, Transformer'larÄ±n RL ile BaÄŸlantÄ±sÄ±"

### âœ… DOÄRU YAKLAÅIM: Ensemble + RL Hibrit Sistem

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VERÄ° KATMANI                              â”‚
â”‚  (Binance, Glassnode, CryptoQuant, Aggr, Coinalyze)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ° (Feature Engineering)     â”‚
â”‚  â€¢ Teknik indikatÃ¶rler (200+ features)                      â”‚
â”‚  â€¢ On-chain metrikler                                        â”‚
â”‚  â€¢ Market microstructure                                     â”‚
â”‚  â€¢ Sentiment scores                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SUPERVISED ML   â”‚   â”‚  UNSUPERVISED ML     â”‚
â”‚  (Tahmin Layer)  â”‚   â”‚  (Pattern/Regime)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ GradBoost      â”‚   â”‚ â€¢ Clustering         â”‚
â”‚ â€¢ XGBoost        â”‚   â”‚ â€¢ Anomaly Detection  â”‚
â”‚ â€¢ LightGBM       â”‚   â”‚ â€¢ PCA/t-SNE          â”‚
â”‚ â€¢ RandomForest   â”‚   â”‚                      â”‚
â”‚                  â”‚   â”‚                      â”‚
â”‚ Ã‡IKIÅ:          â”‚   â”‚ Ã‡IKIÅ:              â”‚
â”‚ - Fiyat tahmin  â”‚   â”‚ - Market regime     â”‚
â”‚ - Trend prob.   â”‚   â”‚ - Pattern clusters  â”‚
â”‚ - Volatility    â”‚   â”‚ - Anomalies         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ENSEMBLE LAYER     â”‚
         â”‚  (Meta-learner)     â”‚
         â”‚  Weighted Combine   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     REINFORCEMENT LEARNING LAYER    â”‚
         â”‚     (Decision Transformer + PPO)    â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  STATE:                             â”‚
         â”‚   - Ensemble tahminleri             â”‚
         â”‚   - Market regime                   â”‚
         â”‚   - Portfolio durumu                â”‚
         â”‚   - Risk metrikleri                 â”‚
         â”‚                                     â”‚
         â”‚  ACTION SPACE:                      â”‚
         â”‚   - LONG (0-100% position)         â”‚
         â”‚   - SHORT (0-100% position)        â”‚
         â”‚   - HOLD                            â”‚
         â”‚   - CLOSE                           â”‚
         â”‚                                     â”‚
         â”‚  REWARD:                            â”‚
         â”‚   R = PnL - Î»â‚Â·Risk - Î»â‚‚Â·DrawDown  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  CLAUDE REASONING       â”‚
         â”‚  (Final Decision)       â”‚
         â”‚  â€¢ RL Ã¶nerisi + context â”‚
         â”‚  â€¢ Risk kontrolÃ¼        â”‚
         â”‚  â€¢ Psikoloji analizi    â”‚
         â”‚  â€¢ RAG knowledge        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”— BAÄLANTI AÃ‡IKLAMASI:

**1. Supervised ML (XGBoost, LightGBM, LSTM, Transformer) ROLÃœ:**
- âŒ DoÄŸrudan trade kararÄ± VERMEZLER
- âœ… RL agent'a **STATE bilgisi** saÄŸlarlar
- âœ… RL agent'Ä±n Ã¶ÄŸrenmesini **hÄ±zlandÄ±rÄ±rlar** (warm start)

**2. RL'nin Supervised ML Ã‡Ä±ktÄ±larÄ±nÄ± Kullanma Åekli:**

```python
# Ã–RNEK STATE VECTOR (RL INPUT)
state = {
    # Supervised tahminler
    "xgb_price_pred_1h": 67234.5,        # XGBoost 1h fiyat tahmini
    "lgbm_trend_prob": 0.73,             # LightGBM trend olasÄ±lÄ±ÄŸÄ±
    "lstm_volatility": 0.042,            # LSTM volatilite tahmini
    "transformer_pattern": "bullish_flag", # Pattern recognition

    # Market features
    "current_price": 67100,
    "rsi_14": 58.3,
    "funding_rate": 0.01,

    # Portfolio state
    "position": 0.5,  # %50 long
    "unrealized_pnl": 234.5,

    # Risk metrics
    "var_95": -1234,
    "sharpe_ratio": 1.8
}

# RL AGENT bu state'e bakarak ACTION karar verir
action = rl_agent.decide(state)  # â†’ "HOLD" veya "INCREASE_LONG" vb.
```

**3. NEDEN BU HÄ°BRÄ°T YAKLAÅIM ÃœSTÃœNDÃœr:**

| YÃ¶ntem | Avantaj | Dezavantaj |
|--------|---------|------------|
| **Sadece Supervised** | Tahmin hÄ±zlÄ±, basit | Dinamik karar verme yok, piyasa deÄŸiÅŸimlerine uyum yavaÅŸ |
| **Sadece RL** | Adaptif, optimal strateji Ã¶ÄŸrenir | Convergence Ã§ok yavaÅŸ, Ã§ok veri gerekir |
| **HÄ°BRÄ°T (Ã–NERÄ°MÄ°Z)** | âœ… Supervised hÄ±zlÄ± insight + RL optimal karar | Kompleks sistem |

---

## ğŸ§  2. RL MEKANÄ°ZMASI: Decision Transformer + PPO Fine-tuning

### A. NEDEN DECISION TRANSFORMER?

**Klasik RL (DQN, PPO) Problemi:**
```
t=0: State â†’ Action â†’ Reward = -5
t=1: State â†’ Action â†’ Reward = -3
t=2: State â†’ Action â†’ Reward = +10  â† Bu reward'a ulaÅŸmak iÃ§in Ã¶nceki -5, -3'Ã¼ gÃ¶rmesi gerek
```
â†’ Kredi atama problemi (credit assignment) kripto'da Ã§ok zor Ã§Ã¼nkÃ¼:
- Position aÃ§ma ve kapama arasÄ±nda 100+ timestep var
- Gecikmeli Ã¶dÃ¼ller (delayed rewards)

**Decision Transformer Ã‡Ã¶zÃ¼mÃ¼:**
```python
# Transformer, geÃ§miÅŸ trajectory'yi bÃ¼tÃ¼nsel gÃ¶rÃ¼r
trajectory = [
    (sâ‚€, aâ‚€, râ‚€),
    (sâ‚, aâ‚, râ‚),
    ...,
    (sâ‚™, aâ‚™, râ‚™)
]

# TARGET RETURN ile koÅŸullandÄ±rma
action = DecisionTransformer(
    states=trajectory_states,
    actions=trajectory_actions,
    returns_to_go=target_return  # â† "Toplam +50% PnL istiyorum"
)
```

### B. MÄ°MARÄ° DETAY

```python
# models/rl/decision_transformer.py

import torch
import torch.nn as nn
from transformers import GPT2Config, GPT2Model

class CryptoDecisionTransformer(nn.Module):
    def __init__(
        self,
        state_dim=256,         # Supervised ML + market features
        action_dim=4,          # LONG, SHORT, HOLD, CLOSE
        hidden_size=768,
        num_layers=6,
        num_heads=8,
        context_length=50      # Son 50 timestep'i hatÄ±rla
    ):
        super().__init__()

        # GPT-2 benzeri transformer backbone
        config = GPT2Config(
            n_embd=hidden_size,
            n_layer=num_layers,
            n_head=num_heads,
            n_positions=context_length * 3  # (s,a,r) Ã¼Ã§lÃ¼leri
        )
        self.transformer = GPT2Model(config)

        # Embedding layers
        self.state_encoder = nn.Linear(state_dim, hidden_size)
        self.action_encoder = nn.Embedding(action_dim, hidden_size)
        self.return_encoder = nn.Linear(1, hidden_size)
        self.timestep_encoder = nn.Embedding(context_length, hidden_size)

        # Output head
        self.action_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, action_dim)
        )

        self.context_length = context_length

    def forward(self, states, actions, returns_to_go, timesteps):
        B, T, _ = states.shape

        # Embeddings
        state_emb = self.state_encoder(states)
        action_emb = self.action_encoder(actions)
        return_emb = self.return_encoder(returns_to_go.unsqueeze(-1))
        time_emb = self.timestep_encoder(timesteps)

        # Interleave: (R_0, s_0, a_0, R_1, s_1, a_1, ...)
        sequence = torch.stack([
            return_emb, state_emb, action_emb
        ], dim=2).reshape(B, 3 * T, -1)

        # Add time embeddings
        sequence = sequence + time_emb.repeat_interleave(3, dim=1)

        # Transformer forward
        transformer_out = self.transformer(inputs_embeds=sequence).last_hidden_state

        # Extract action predictions
        action_logits = self.action_head(transformer_out[:, 1::3])  # s pozisyonlarÄ±

        return action_logits


# PPO Fine-tuning wrapper
class PPOFineTuner:
    def __init__(self, dt_model, learning_rate=3e-5):
        self.dt_model = dt_model
        self.optimizer = torch.optim.AdamW(dt_model.parameters(), lr=learning_rate)
        self.clip_epsilon = 0.2

    def update(self, trajectories, advantages):
        """PPO objective ile fine-tune"""
        for epoch in range(4):  # PPO epochs
            for batch in trajectories:
                # Compute policy ratio
                old_log_probs = batch['log_probs']
                new_logits = self.dt_model(
                    batch['states'],
                    batch['actions'],
                    batch['returns_to_go'],
                    batch['timesteps']
                )
                new_log_probs = torch.log_softmax(new_logits, dim=-1)

                ratio = torch.exp(new_log_probs - old_log_probs)

                # PPO clipped objective
                clipped_ratio = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon)
                loss = -torch.min(
                    ratio * advantages,
                    clipped_ratio * advantages
                ).mean()

                # Backward
                self.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.dt_model.parameters(), 1.0)
                self.optimizer.step()
```

### C. EÄÄ°TÄ°M SÃœRECÄ°

```python
# training/rl_training.py

def train_decision_transformer():
    """
    ÃœÃ‡ AÅAMALI EÄÄ°TÄ°M
    """

    # AÅAMA 1: Offline Pre-training (Historical Data)
    print("AÅAMA 1: Offline BC (Behavioral Cloning) - Expert demonstrations")
    # GeÃ§miÅŸ baÅŸarÄ±lÄ± trade'leri taklit et
    for epoch in range(100):
        for batch in expert_trajectories:  # Ä°yi performans gÃ¶steren geÃ§miÅŸ trade'ler
            loss = behavioral_cloning_loss(model, batch)
            loss.backward()
            optimizer.step()

    # AÅAMA 2: Online Fine-tuning (SimÃ¼le Backtest)
    print("AÅAMA 2: Online RL - Simulated trading")
    env = BacktestEnv(historical_data)  # SimÃ¼lasyon ortamÄ±
    for episode in range(10000):
        trajectory = collect_trajectory(env, model, target_return=0.15)  # %15 hedef
        returns = compute_returns(trajectory)

        # Decision Transformer gÃ¼ncelle
        model_loss = dt_loss(model, trajectory, returns)
        model_loss.backward()
        optimizer.step()

    # AÅAMA 3: PPO Fine-tuning (Risk-adjusted optimization)
    print("AÅAMA 3: PPO fine-tuning - Risk kontrolÃ¼")
    ppo = PPOFineTuner(model)
    for iteration in range(1000):
        trajectories = collect_multiple_trajectories(env, model, n=16)
        advantages = compute_gae(trajectories)  # Generalized Advantage Estimation
        ppo.update(trajectories, advantages)
```

---

## ğŸ’¾ 3. BELLEK VE STATE MANAGEMENT Ã–NERÄ°SÄ°

### A. HÄ°BRÄ°T BELLEK SÄ°STEMÄ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Ã‡OKLU BELLEK SÄ°STEMÄ°                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  SHORT-TERM      â”‚  â”‚  WORKING MEMORY  â”‚                â”‚
â”‚  â”‚  (Redis)         â”‚  â”‚  (In-Memory)     â”‚                â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚  â”‚ â€¢ Live prices    â”‚  â”‚ â€¢ Current state  â”‚                â”‚
â”‚  â”‚ â€¢ Recent trades  â”‚  â”‚ â€¢ Active orders  â”‚                â”‚
â”‚  â”‚ â€¢ Order book     â”‚  â”‚ â€¢ Risk metrics   â”‚                â”‚
â”‚  â”‚ TTL: 1 hour      â”‚  â”‚ Volatile         â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  MEDIUM-TERM (PostgreSQL TimescaleDB)â”‚                   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚  â”‚ â€¢ OHLCV candles (compress after 30d) â”‚                   â”‚
â”‚  â”‚ â€¢ Indicators (RSI, MACD, etc.)       â”‚                   â”‚
â”‚  â”‚ â€¢ Trade history                       â”‚                   â”‚
â”‚  â”‚ â€¢ Model predictions                   â”‚                   â”‚
â”‚  â”‚ Retention: 2 years                    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚  LONG-TERM KNOWLEDGE (Vector DB)     â”‚                   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                   â”‚
â”‚  â”‚ â€¢ RAG embeddings (PDF iÃ§erikler)     â”‚                   â”‚
â”‚  â”‚ â€¢ Pattern library                     â”‚                   â”‚
â”‚  â”‚ â€¢ Historical regime data              â”‚                   â”‚
â”‚  â”‚ â€¢ Model checkpoints                   â”‚                   â”‚
â”‚  â”‚ DB: Pinecone / Weaviate              â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### B. IMPLEMENTATION

```python
# core/memory/memory_manager.py

from redis import Redis
from sqlalchemy import create_engine
from pinecone import Pinecone
import pandas as pd

class MemoryManager:
    """Hibrit bellek yÃ¶neticisi"""

    def __init__(self):
        # Short-term: Redis
        self.redis = Redis(
            host='localhost',
            port=6379,
            decode_responses=True
        )

        # Medium-term: TimescaleDB
        self.timescale = create_engine('postgresql://user:pass@localhost/sigma_db')

        # Long-term: Pinecone
        self.vector_db = Pinecone(api_key="...")
        self.index = self.vector_db.Index("sigma-knowledge")

        # In-memory cache
        self.working_memory = {
            'current_state': {},
            'active_positions': {},
            'pending_orders': []
        }

    # ===== SHORT-TERM OPERATIONS =====
    def cache_price(self, symbol: str, price: float, ttl: int = 3600):
        """Cache live price with TTL"""
        self.redis.setex(f"price:{symbol}", ttl, price)

    def get_recent_trades(self, symbol: str, limit: int = 100):
        """Get recent trades from Redis"""
        return self.redis.lrange(f"trades:{symbol}", 0, limit - 1)

    # ===== MEDIUM-TERM OPERATIONS =====
    def store_candle(self, symbol: str, timeframe: str, candle: dict):
        """Store OHLCV to TimescaleDB"""
        query = """
        INSERT INTO candles (symbol, timeframe, timestamp, open, high, low, close, volume)
        VALUES (%(symbol)s, %(timeframe)s, %(timestamp)s, %(open)s, %(high)s, %(low)s, %(close)s, %(volume)s)
        ON CONFLICT (symbol, timeframe, timestamp) DO UPDATE SET
            open = EXCLUDED.open,
            high = EXCLUDED.high,
            low = EXCLUDED.low,
            close = EXCLUDED.close,
            volume = EXCLUDED.volume
        """
        with self.timescale.connect() as conn:
            conn.execute(query, {
                'symbol': symbol,
                'timeframe': timeframe,
                **candle
            })

    def get_historical_candles(self, symbol: str, timeframe: str,
                               start: str, end: str) -> pd.DataFrame:
        """Retrieve historical OHLCV"""
        query = """
        SELECT * FROM candles
        WHERE symbol = %(symbol)s
          AND timeframe = %(timeframe)s
          AND timestamp BETWEEN %(start)s AND %(end)s
        ORDER BY timestamp ASC
        """
        return pd.read_sql(query, self.timescale, params={
            'symbol': symbol,
            'timeframe': timeframe,
            'start': start,
            'end': end
        })

    # ===== LONG-TERM KNOWLEDGE =====
    def store_pattern(self, pattern_name: str, embedding: list, metadata: dict):
        """Store learned pattern to vector DB"""
        self.index.upsert(vectors=[{
            'id': pattern_name,
            'values': embedding,
            'metadata': metadata
        }])

    def search_similar_patterns(self, current_embedding: list, top_k: int = 5):
        """RAG: Find similar historical patterns"""
        results = self.index.query(
            vector=current_embedding,
            top_k=top_k,
            include_metadata=True
        )
        return results['matches']

    # ===== WORKING MEMORY =====
    def update_state(self, key: str, value):
        """Update current agent state"""
        self.working_memory['current_state'][key] = value

    def get_state(self) -> dict:
        """Get full current state"""
        return {
            **self.working_memory['current_state'],
            'positions': self.working_memory['active_positions'],
            'orders': self.working_memory['pending_orders']
        }
```

### C. STATE PERSISTENCE STRATEGY

```python
# core/state/state_manager.py

class StatePersistence:
    """Agent state'ini gÃ¼venli ÅŸekilde sakla/geri yÃ¼kle"""

    def save_checkpoint(self, agent_state: dict, checkpoint_id: str):
        """
        Checkpoint iÃ§eriÄŸi:
        - Model weights
        - Optimizer state
        - Memory buffers
        - Performance metrics
        """
        checkpoint = {
            'timestamp': datetime.utcnow(),
            'agent_state': agent_state,
            'model_state': self.model.state_dict(),
            'optimizer_state': self.optimizer.state_dict(),
            'metrics': self.get_current_metrics()
        }

        # S3 / local disk'e kaydet
        torch.save(checkpoint, f"checkpoints/{checkpoint_id}.pt")

    def load_checkpoint(self, checkpoint_id: str):
        """Checkpoint'ten geri yÃ¼kle"""
        checkpoint = torch.load(f"checkpoints/{checkpoint_id}.pt")

        self.model.load_state_dict(checkpoint['model_state'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state'])

        return checkpoint['agent_state']
```

---

## ğŸ”¬ 4. BACKTEST SÄ°STEMÄ° MÄ°MARÄ°SÄ°

### A. EVENT-DRIVEN BACKTEST ENGINE

```python
# backtest/engine.py

from typing import List, Dict
from dataclasses import dataclass
from datetime import datetime
import pandas as pd

@dataclass
class Order:
    symbol: str
    side: str  # 'buy' / 'sell'
    quantity: float
    price: float
    timestamp: datetime
    order_type: str  # 'market' / 'limit'

@dataclass
class Position:
    symbol: str
    quantity: float
    entry_price: float
    current_price: float
    unrealized_pnl: float

class BacktestEngine:
    """
    GerÃ§ekÃ§i backtest ortamÄ±:
    - Slippage simulation
    - Commission fees
    - Market impact modeling
    - Partial fills
    """

    def __init__(
        self,
        initial_capital: float = 100000,
        commission_rate: float = 0.0004,  # %0.04
        slippage_model: str = 'volume_based'
    ):
        self.capital = initial_capital
        self.initial_capital = initial_capital
        self.commission_rate = commission_rate
        self.slippage_model = slippage_model

        self.positions: Dict[str, Position] = {}
        self.orders: List[Order] = []
        self.trades: List[Dict] = []

        self.equity_curve = []
        self.current_time = None

    def load_data(self, symbols: List[str], start: str, end: str):
        """Mum verilerini yÃ¼kle"""
        self.data = {}
        for symbol in symbols:
            # TimescaleDB'den Ã§ek
            df = memory_manager.get_historical_candles(
                symbol=symbol,
                timeframe='1m',  # En dÃ¼ÅŸÃ¼k granÃ¼larite
                start=start,
                end=end
            )
            self.data[symbol] = df

    def calculate_slippage(self, symbol: str, side: str, quantity: float) -> float:
        """
        GerÃ§ekÃ§i slippage hesapla
        - Market depth'e gÃ¶re
        - Volatility'ye gÃ¶re
        """
        if self.slippage_model == 'volume_based':
            current_bar = self.data[symbol].loc[self.current_time]
            volume = current_bar['volume']

            # EÄŸer iÅŸlem hacmi toplam hacmin %1'inden fazlaysa impact var
            impact_ratio = (quantity / volume) * 100
            slippage_pct = min(impact_ratio * 0.1, 0.5)  # Max %0.5 slippage

            return slippage_pct / 100

        return 0.0001  # Sabit %0.01 default

    def execute_order(self, order: Order) -> bool:
        """Emir Ã§alÄ±ÅŸtÄ±r (komisyon + slippage ile)"""
        slippage = self.calculate_slippage(order.symbol, order.side, order.quantity)

        if order.side == 'buy':
            execution_price = order.price * (1 + slippage)
        else:
            execution_price = order.price * (1 - slippage)

        commission = order.quantity * execution_price * self.commission_rate
        total_cost = (order.quantity * execution_price) + commission

        # Sermaye kontrolÃ¼
        if order.side == 'buy' and total_cost > self.capital:
            print(f"Insufficient capital for {order}")
            return False

        # Position gÃ¼ncelle
        if order.side == 'buy':
            if order.symbol in self.positions:
                pos = self.positions[order.symbol]
                new_quantity = pos.quantity + order.quantity
                new_entry = (pos.entry_price * pos.quantity + execution_price * order.quantity) / new_quantity
                pos.quantity = new_quantity
                pos.entry_price = new_entry
            else:
                self.positions[order.symbol] = Position(
                    symbol=order.symbol,
                    quantity=order.quantity,
                    entry_price=execution_price,
                    current_price=execution_price,
                    unrealized_pnl=0
                )

            self.capital -= total_cost

        else:  # sell
            if order.symbol not in self.positions:
                print(f"No position to sell for {order.symbol}")
                return False

            pos = self.positions[order.symbol]
            sell_quantity = min(order.quantity, pos.quantity)

            realized_pnl = (execution_price - pos.entry_price) * sell_quantity - commission
            self.capital += sell_quantity * execution_price - commission

            pos.quantity -= sell_quantity
            if pos.quantity <= 0:
                del self.positions[order.symbol]

            self.trades.append({
                'timestamp': self.current_time,
                'symbol': order.symbol,
                'side': 'sell',
                'quantity': sell_quantity,
                'price': execution_price,
                'pnl': realized_pnl
            })

        return True

    def run(self, agent, start_date: str, end_date: str):
        """Backtest dÃ¶ngÃ¼sÃ¼"""
        timestamps = pd.date_range(start_date, end_date, freq='1min')

        for ts in timestamps:
            self.current_time = ts

            # PozisyonlarÄ± gÃ¼ncelle
            for symbol, pos in self.positions.items():
                try:
                    current_price = self.data[symbol].loc[ts, 'close']
                    pos.current_price = current_price
                    pos.unrealized_pnl = (current_price - pos.entry_price) * pos.quantity
                except KeyError:
                    continue

            # Toplam equity hesapla
            total_equity = self.capital + sum(
                pos.quantity * pos.current_price for pos in self.positions.values()
            )
            self.equity_curve.append({
                'timestamp': ts,
                'equity': total_equity,
                'cash': self.capital,
                'positions_value': total_equity - self.capital
            })

            # Agent'tan state al
            state = self.build_state(ts)

            # Agent karar ver
            action = agent.decide(state)

            # Action'Ä± emirlere Ã§evir
            if action['type'] == 'LONG':
                order = Order(
                    symbol=action['symbol'],
                    side='buy',
                    quantity=action['quantity'],
                    price=self.data[action['symbol']].loc[ts, 'close'],
                    timestamp=ts,
                    order_type='market'
                )
                self.execute_order(order)

            elif action['type'] == 'SHORT':
                # Short iÃ§in satÄ±ÅŸ
                pass

            # Risk kontrolÃ¼ - her 1000 bar'da checkpoint
            if len(self.equity_curve) % 1000 == 0:
                self.save_checkpoint()

    def build_state(self, timestamp) -> dict:
        """Agent iÃ§in state oluÅŸtur"""
        # Teknik indikatÃ¶rler, on-chain vb. hesapla
        state = {
            'timestamp': timestamp,
            'prices': {},
            'indicators': {},
            'positions': self.positions,
            'capital': self.capital
        }

        for symbol in self.data.keys():
            try:
                current_bar = self.data[symbol].loc[timestamp]
                state['prices'][symbol] = current_bar['close']
                # ... daha fazla feature
            except KeyError:
                continue

        return state

    def get_performance_metrics(self) -> dict:
        """Performans metrikleri"""
        equity_df = pd.DataFrame(self.equity_curve)

        returns = equity_df['equity'].pct_change().dropna()

        total_return = (equity_df['equity'].iloc[-1] / self.initial_capital - 1) * 100
        sharpe = returns.mean() / returns.std() * (365 * 24 * 60) ** 0.5  # YÄ±llÄ±k Sharpe

        max_drawdown = self.calculate_max_drawdown(equity_df['equity'])

        win_trades = [t for t in self.trades if t.get('pnl', 0) > 0]
        win_rate = len(win_trades) / len(self.trades) if self.trades else 0

        return {
            'total_return_pct': total_return,
            'sharpe_ratio': sharpe,
            'max_drawdown_pct': max_drawdown,
            'win_rate': win_rate,
            'total_trades': len(self.trades),
            'final_equity': equity_df['equity'].iloc[-1]
        }

    def calculate_max_drawdown(self, equity_series):
        """Maximum drawdown hesapla"""
        cummax = equity_series.cummax()
        drawdown = (equity_series - cummax) / cummax * 100
        return drawdown.min()
```

### B. WALK-FORWARD OPTIMIZATION

```python
# backtest/walk_forward.py

class WalkForwardOptimizer:
    """
    Rolling window ile model optimization

    â”œâ”€ Train â”€â”€â”¤â”€ Validate â”€â”¤
              â”œâ”€ Train â”€â”€â”¤â”€ Validate â”€â”¤
                        â”œâ”€ Train â”€â”€â”¤â”€ Validate â”€â”¤
    """

    def __init__(self, train_period_days=180, test_period_days=30):
        self.train_period = train_period_days
        self.test_period = test_period_days

    def run(self, start_date, end_date, agent):
        """Walk-forward test"""
        results = []

        current_start = pd.to_datetime(start_date)
        end = pd.to_datetime(end_date)

        while current_start < end:
            train_end = current_start + pd.Timedelta(days=self.train_period)
            test_end = train_end + pd.Timedelta(days=self.test_period)

            if test_end > end:
                break

            print(f"Training: {current_start} to {train_end}")
            # Agent'Ä± train et
            agent.train(start=current_start, end=train_end)

            print(f"Testing: {train_end} to {test_end}")
            # Test et
            backtest = BacktestEngine()
            backtest.load_data(['BTCUSDT'], train_end, test_end)
            backtest.run(agent, train_end, test_end)

            metrics = backtest.get_performance_metrics()
            results.append({
                'test_period_start': train_end,
                'test_period_end': test_end,
                **metrics
            })

            # Sonraki window'a geÃ§
            current_start = train_end

        return pd.DataFrame(results)
```

---

## ğŸ“š 5. VERÄ° KAYNAÄI ENTEGRASYONU

### A. AGGREGATOR PATTERN

```python
# data/sources/aggregator.py

from abc import ABC, abstractmethod
from typing import List, Dict
import asyncio

class DataSource(ABC):
    """Abstract base class for data sources"""

    @abstractmethod
    async def fetch_ohlcv(self, symbol: str, timeframe: str, limit: int):
        pass

    @abstractmethod
    async def fetch_orderbook(self, symbol: str, depth: int):
        pass

class BinanceSource(DataSource):
    async def fetch_ohlcv(self, symbol, timeframe, limit):
        # Binance API Ã§aÄŸrÄ±sÄ±
        pass

class GlassnodeSource(DataSource):
    async def fetch_onchain_metric(self, metric: str):
        # Glassnode API
        pass

# ... DiÄŸer kaynaklar (OKX, Bybit, CryptoQuant, vb.)

class DataAggregator:
    """TÃ¼m kaynaklardan veri topla ve birleÅŸtir"""

    def __init__(self):
        self.sources = {
            'binance': BinanceSource(),
            'okx': OKXSource(),
            'glassnode': GlassnodeSource(),
            'cryptoquant': CryptoQuantSource(),
            'aggr': AggrSource(),  # tucsky/aggr entegrasyonu
            'coinalyze': CoinalyzeSource()
        }

    async def fetch_all_ohlcv(self, symbol: str, timeframe: str) -> pd.DataFrame:
        """Paralel olarak tÃ¼m kaynaklardan Ã§ek, birleÅŸtir"""
        tasks = [
            source.fetch_ohlcv(symbol, timeframe, 500)
            for name, source in self.sources.items()
            if hasattr(source, 'fetch_ohlcv')
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Merge ve outlier filtering
        combined_df = self.merge_ohlcv(results)
        return combined_df

    def merge_ohlcv(self, dataframes: List[pd.DataFrame]) -> pd.DataFrame:
        """
        Birden fazla kaynaktan gelen OHLCV'yi birleÅŸtir
        - Timestamp alignment
        - Median price kullan (outlier'lara karÅŸÄ± robust)
        """
        merged = pd.concat(dataframes, axis=0)
        merged = merged.groupby('timestamp').agg({
            'open': 'median',
            'high': 'max',
            'low': 'min',
            'close': 'median',
            'volume': 'sum'
        })
        return merged
```

### B. TUCSKY/AGGR ENTEGRASYONU

```python
# data/sources/aggr_client.py

import websocket
import json

class AggrWebSocketClient:
    """
    tucsky/aggr WebSocket client
    Real-time trade aggregation
    """

    def __init__(self, url="wss://api.aggr.trade"):
        self.url = url
        self.ws = None
        self.subscriptions = []

    def connect(self):
        self.ws = websocket.WebSocketApp(
            self.url,
            on_message=self.on_message,
            on_error=self.on_error,
            on_close=self.on_close
        )
        self.ws.on_open = self.on_open
        self.ws.run_forever()

    def on_open(self, ws):
        # Subscribe to symbols
        subscribe_msg = {
            "op": "subscribe",
            "args": ["BINANCE:BTCUSDT", "OKEX:BTC-USDT"]
        }
        ws.send(json.dumps(subscribe_msg))

    def on_message(self, ws, message):
        data = json.loads(message)

        # Process aggregated trade data
        if data['type'] == 'trade':
            self.handle_trade(data)

        elif data['type'] == 'liquidation':
            self.handle_liquidation(data)

    def handle_trade(self, trade_data):
        """
        Aggregated trade verisi:
        - Exchange
        - Price
        - Size
        - Side (buy/sell)
        - Timestamp
        """
        # Redis'e yaz
        memory_manager.redis.lpush(
            f"trades:{trade_data['symbol']}",
            json.dumps(trade_data)
        )
        memory_manager.redis.ltrim(f"trades:{trade_data['symbol']}", 0, 999)  # Son 1000
```

---

## ğŸ¨ 6. REACT UI MÄ°MARÄ°SÄ°

### A. COMPONENT HIERARCHY

```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ Dashboard.tsx          # Ana dashboard
â”‚   â”‚   â”œâ”€â”€ Analysis.tsx           # DetaylÄ± analiz sayfasÄ±
â”‚   â”‚   â”œâ”€â”€ Backtest.tsx           # Backtest interface
â”‚   â”‚   â”œâ”€â”€ Settings.tsx           # Ayarlar
â”‚   â”‚   â””â”€â”€ Alerts.tsx             # UyarÄ±lar
â”‚   â”‚
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ TopBar.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Layout.tsx
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ charts/
â”‚   â”‚   â”‚   â”œâ”€â”€ TradingViewChart.tsx     # Ana fiyat grafiÄŸi
â”‚   â”‚   â”‚   â”œâ”€â”€ EquityCurve.tsx          # Equity curve
â”‚   â”‚   â”‚   â”œâ”€â”€ HeatMap.tsx              # Korelasyon/flow heatmap
â”‚   â”‚   â”‚   â””â”€â”€ VolumeProfile.tsx
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ widgets/
â”‚   â”‚   â”‚   â”œâ”€â”€ MarketPulse.tsx          # Genel sentiment
â”‚   â”‚   â”‚   â”œâ”€â”€ FlowWidget.tsx           # Nakit akÄ±ÅŸÄ±
â”‚   â”‚   â”‚   â”œâ”€â”€ OnChainMetrics.tsx       # On-chain data
â”‚   â”‚   â”‚   â”œâ”€â”€ ScenarioCard.tsx         # Bull/Base/Bear senaryolar
â”‚   â”‚   â”‚   â”œâ”€â”€ AlertsList.tsx           # Aktif uyarÄ±lar
â”‚   â”‚   â”‚   â””â”€â”€ PositionsTable.tsx       # AÃ§Ä±k pozisyonlar
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ common/
â”‚   â”‚       â”œâ”€â”€ Card.tsx
â”‚   â”‚       â”œâ”€â”€ Button.tsx
â”‚   â”‚       â”œâ”€â”€ Badge.tsx
â”‚   â”‚       â””â”€â”€ Modal.tsx
â”‚   â”‚
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useWebSocket.ts              # Real-time data
â”‚   â”‚   â”œâ”€â”€ useBacktest.ts
â”‚   â”‚   â””â”€â”€ useAnalysis.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ api.ts                       # Backend API client
â”‚   â”‚   â””â”€â”€ websocket.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ store/                           # Redux/Zustand
â”‚   â”‚   â”œâ”€â”€ slices/
â”‚   â”‚   â”‚   â”œâ”€â”€ marketSlice.ts
â”‚   â”‚   â”‚   â”œâ”€â”€ analysisSlice.ts
â”‚   â”‚   â”‚   â””â”€â”€ backtestSlice.ts
â”‚   â”‚   â””â”€â”€ store.ts
â”‚   â”‚
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ index.ts                     # TypeScript types
```

### B. UI MOCKUP & DESIGN

```typescript
// src/pages/Dashboard.tsx

import React from 'react';
import { Grid, Card, Typography } from '@mui/material';
import TradingViewChart from '../components/charts/TradingViewChart';
import MarketPulse from '../components/widgets/MarketPulse';
import FlowWidget from '../components/widgets/FlowWidget';
import ScenarioCard from '../components/widgets/ScenarioCard';

const Dashboard: React.FC = () => {
  return (
    <Grid container spacing={3}>
      {/* Top Row: Market Pulse + Key Metrics */}
      <Grid item xs={12} md={8}>
        <MarketPulse />
      </Grid>
      <Grid item xs={12} md={4}>
        <FlowWidget />
      </Grid>

      {/* Main Chart */}
      <Grid item xs={12} lg={8}>
        <Card sx={{ height: 600 }}>
          <TradingViewChart symbol="BTCUSDT" />
        </Card>
      </Grid>

      {/* Scenarios */}
      <Grid item xs={12} lg={4}>
        <Grid container spacing={2}>
          <Grid item xs={12}>
            <ScenarioCard type="bull" />
          </Grid>
          <Grid item xs={12}>
            <ScenarioCard type="base" />
          </Grid>
          <Grid item xs={12}>
            <ScenarioCard type="bear" />
          </Grid>
        </Grid>
      </Grid>

      {/* On-Chain Metrics */}
      <Grid item xs={12}>
        <OnChainMetrics />
      </Grid>
    </Grid>
  );
};

export default Dashboard;
```

### C. REAL-TIME DATA FLOW

```typescript
// src/hooks/useWebSocket.ts

import { useEffect, useState } from 'react';
import { io, Socket } from 'socket.io-client';

interface MarketData {
  symbol: string;
  price: number;
  change_24h: number;
  volume: number;
  timestamp: string;
}

export const useWebSocket = (url: string) => {
  const [socket, setSocket] = useState<Socket | null>(null);
  const [marketData, setMarketData] = useState<MarketData | null>(null);
  const [alerts, setAlerts] = useState<any[]>([]);

  useEffect(() => {
    const socketInstance = io(url);

    socketInstance.on('connect', () => {
      console.log('WebSocket connected');
    });

    // Real-time price updates
    socketInstance.on('market_update', (data: MarketData) => {
      setMarketData(data);
    });

    // Alert notifications
    socketInstance.on('alert', (alert) => {
      setAlerts((prev) => [alert, ...prev].slice(0, 50));
    });

    setSocket(socketInstance);

    return () => {
      socketInstance.disconnect();
    };
  }, [url]);

  return { socket, marketData, alerts };
};
```

---

## ğŸ¯ 7. PDF Ã–ÄRENME SÄ°STEMÄ° (RAG)

```python
# learning/pdf_processor.py

import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone

class PDFLearningSystem:
    """
    PDF'lerden Ã¶ÄŸrenen RAG sistemi
    """

    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        self.vector_store = Pinecone.from_existing_index("sigma-knowledge", self.embeddings)

    def ingest_pdf(self, pdf_path: str, metadata: dict = None):
        """PDF'i oku ve vector DB'ye ekle"""
        # PDF oku
        with open(pdf_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()

        # Chunk'lara bÃ¶l
        chunks = self.text_splitter.split_text(text)

        # Embed ve kaydet
        self.vector_store.add_texts(
            texts=chunks,
            metadatas=[{
                'source': pdf_path,
                **(metadata or {})
            }] * len(chunks)
        )

        print(f"âœ… {pdf_path} ingested: {len(chunks)} chunks")

    def query(self, question: str, k: int = 5) -> List[str]:
        """Bilgi bankasÄ±ndan ilgili bilgileri Ã§ek"""
        docs = self.vector_store.similarity_search(question, k=k)
        return [doc.page_content for doc in docs]

# KullanÄ±m
pdf_learner = PDFLearningSystem()
pdf_learner.ingest_pdf("books/technical_analysis.pdf", {"category": "technical"})
pdf_learner.ingest_pdf("books/market_psychology.pdf", {"category": "psychology"})

# Agent karar verirken:
context = pdf_learner.query("What is the golden zone in Fibonacci retracement?")
# â†’ RL agent'a ekstra context olarak ver
```

---

## ğŸš€ 8. DEPLOYMENT MÄ°MARÄ°SÄ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       PRODUCTION STACK                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   NGINX     â”‚â”€â”€â”€â”€â”€â–¶â”‚  Frontend   â”‚     â”‚  Monitoring â”‚  â”‚
â”‚  â”‚  (Reverse   â”‚      â”‚  (React)    â”‚     â”‚  (Grafana)  â”‚  â”‚
â”‚  â”‚   Proxy)    â”‚      â”‚  Port: 3000 â”‚     â”‚             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚     FastAPI Backend             â”‚                        â”‚
â”‚  â”‚     (Uvicorn + Gunicorn)        â”‚                        â”‚
â”‚  â”‚     Port: 8000                  â”‚                        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚                        â”‚
â”‚  â”‚  â”‚  /api/analysis           â”‚   â”‚                        â”‚
â”‚  â”‚  â”‚  /api/backtest           â”‚   â”‚                        â”‚
â”‚  â”‚  â”‚  /ws/realtime            â”‚   â”‚                        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚           â”‚                                                  â”‚
â”‚           â–¼                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚       WORKER SERVICES (Celery)          â”‚                â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚  â”‚  â€¢ Data ingestion workers               â”‚                â”‚
â”‚  â”‚  â€¢ ML training workers                  â”‚                â”‚
â”‚  â”‚  â€¢ Backtest workers                     â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚             â”‚                                                â”‚
â”‚             â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚         DATA LAYER                      â”‚                â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                â”‚
â”‚  â”‚  Redis (Cache + Queue)                  â”‚                â”‚
â”‚  â”‚  PostgreSQL + TimescaleDB               â”‚                â”‚
â”‚  â”‚  Pinecone (Vector DB)                   â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ 9. PROJE KLASÃ–R YAPISI (FÄ°NAL)

```
sigma-analyst/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”‚   â”œâ”€â”€ analysis.py
â”‚   â”‚   â”‚   â”œâ”€â”€ backtest.py
â”‚   â”‚   â”‚   â”œâ”€â”€ data.py
â”‚   â”‚   â”‚   â””â”€â”€ alerts.py
â”‚   â”‚   â”œâ”€â”€ dependencies.py
â”‚   â”‚   â””â”€â”€ main.py              # FastAPI app
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py            # Settings
â”‚   â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”‚   â”œâ”€â”€ memory_manager.py
â”‚   â”‚   â”‚   â””â”€â”€ state_manager.py
â”‚   â”‚   â””â”€â”€ agents/
â”‚   â”‚       â”œâ”€â”€ sigma_agent.py   # Main agent
â”‚   â”‚       â””â”€â”€ roles/
â”‚   â”‚           â”œâ”€â”€ analyst.py
â”‚   â”‚           â”œâ”€â”€ intelligence.py
â”‚   â”‚           â””â”€â”€ psychologist.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”‚   â”œâ”€â”€ ensemble.py      # GradBoost, XGBoost, LightGBM
â”‚   â”‚   â”‚   â”œâ”€â”€ lstm.py
â”‚   â”‚   â”‚   â””â”€â”€ transformer.py
â”‚   â”‚   â”œâ”€â”€ rl/
â”‚   â”‚   â”‚   â”œâ”€â”€ decision_transformer.py
â”‚   â”‚   â”‚   â””â”€â”€ ppo.py
â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚       â””â”€â”€ report.py        # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ sources/
â”‚   â”‚   â”‚   â”œâ”€â”€ binance.py
â”‚   â”‚   â”‚   â”œâ”€â”€ glassnode.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cryptoquant.py
â”‚   â”‚   â”‚   â”œâ”€â”€ aggr.py
â”‚   â”‚   â”‚   â””â”€â”€ aggregator.py
â”‚   â”‚   â”œâ”€â”€ processors/
â”‚   â”‚   â”‚   â”œâ”€â”€ technical_indicators.py
â”‚   â”‚   â”‚   â”œâ”€â”€ onchain.py
â”‚   â”‚   â”‚   â””â”€â”€ features.py
â”‚   â”‚   â””â”€â”€ downloaders/
â”‚   â”‚       â””â”€â”€ candle_downloader.py
â”‚   â”‚
â”‚   â”œâ”€â”€ backtest/
â”‚   â”‚   â”œâ”€â”€ engine.py
â”‚   â”‚   â”œâ”€â”€ walk_forward.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â”‚
â”‚   â”œâ”€â”€ learning/
â”‚   â”‚   â”œâ”€â”€ pdf_processor.py     # RAG
â”‚   â”‚   â”œâ”€â”€ trainer.py           # RL training
â”‚   â”‚   â””â”€â”€ evaluator.py
â”‚   â”‚
â”‚   â”œâ”€â”€ tasks/                   # Celery tasks
â”‚   â”‚   â”œâ”€â”€ data_tasks.py
â”‚   â”‚   â”œâ”€â”€ training_tasks.py
â”‚   â”‚   â””â”€â”€ monitoring_tasks.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pages/               # (yukarÄ±da detaylandÄ±rÄ±ldÄ±)
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ store/
â”‚   â”‚   â””â”€â”€ types/
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter research notebooks
â”‚   â”œâ”€â”€ exploratory/
â”‚   â””â”€â”€ experiments/
â”‚
â”œâ”€â”€ data/                        # Local data storage
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ models/                  # Saved model checkpoints
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ API.md
â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â””â”€â”€ LEARNING.md
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ backtests/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## âœ… 10. Ã–NCELÄ°KLENDÄ°RÄ°LMÄ°Å UYGULAMA PLANI

### FAZA 1: TEMEL ALT YAPI (Hafta 1-2)
1. âœ… Proje yapÄ±sÄ± oluÅŸtur
2. âœ… Veri kaynaklarÄ±nÄ± entegre et (Binance, Glassnode API)
3. âœ… Memory Manager (Redis + PostgreSQL) kur
4. âœ… Basit FastAPI backend
5. âœ… React frontend iskelet

### FAZA 2: ML PÄ°PELÄ°NE (Hafta 3-4)
1. âœ… Feature engineering (teknik indikatÃ¶rler)
2. âœ… GradBoost + XGBoost + LightGBM ensemble
3. âœ… Backtest engine (basit)
4. âœ… Model eÄŸitimi pipeline

### FAZA 3: RL SÄ°STEMÄ° (Hafta 5-6)
1. âœ… Decision Transformer implementasyonu
2. âœ… PPO fine-tuning
3. âœ… Offline BC pre-training
4. âœ… Walk-forward testing

### FAZA 4: CLAUDE ENTEGRASYONu (Hafta 7)
1. âœ… PDF RAG sistemi
2. âœ… Claude Reasoning API entegrasyonu
3. âœ… Multi-role prompt engineering
4. âœ… JSON output formatting

### FAZA 5: UI & DEPLOYMENT (Hafta 8-9)
1. âœ… React dashboard tamamla
2. âœ… WebSocket real-time data
3. âœ… Docker containerization
4. âœ… Production deployment

---

## ğŸ“ 11. Ã–ÄRENME METRÄ°KLERÄ° & EVALUATION

```python
# learning/evaluator.py

class ModelEvaluator:
    """
    Model performansÄ±nÄ± sÃ¼rekli izle
    """

    def evaluate_ensemble(self, y_true, y_pred_ensemble):
        """Ensemble tahmin kalitesi"""
        from sklearn.metrics import mean_absolute_error, r2_score

        mae = mean_absolute_error(y_true, y_pred_ensemble)
        r2 = r2_score(y_true, y_pred_ensemble)

        return {
            'mae': mae,
            'r2': r2,
            'mape': np.mean(np.abs((y_true - y_pred_ensemble) / y_true)) * 100
        }

    def evaluate_rl_policy(self, trajectories):
        """RL policy performansÄ±"""
        returns = [sum(t['rewards']) for t in trajectories]
        sharpes = [self.calculate_sharpe(t) for t in trajectories]

        return {
            'avg_return': np.mean(returns),
            'avg_sharpe': np.mean(sharpes),
            'win_rate': sum(r > 0 for r in returns) / len(returns)
        }

    def calibration_check(self, predicted_probs, outcomes):
        """
        Model kalibrasyonu - tahmin edilen olasÄ±lÄ±klar gerÃ§ekle uyumlu mu?
        """
        from sklearn.calibration import calibration_curve

        prob_true, prob_pred = calibration_curve(outcomes, predicted_probs, n_bins=10)

        # Brier Score (dÃ¼ÅŸÃ¼k = iyi)
        brier = np.mean((predicted_probs - outcomes) ** 2)

        return {
            'brier_score': brier,
            'calibration_curve': (prob_true, prob_pred)
        }
```

---

## ğŸ”’ 12. RÄ°SK YÃ–NETÄ°MÄ° & GÃœVENLÄ°K

```python
# core/risk/risk_manager.py

class RiskManager:
    """
    Trade seviyesinde risk kontrolÃ¼
    """

    def __init__(self, max_position_size_pct=0.2, max_leverage=3, max_drawdown_pct=0.15):
        self.max_position_size_pct = max_position_size_pct  # PortfÃ¶yÃ¼n max %20'si
        self.max_leverage = max_leverage
        self.max_drawdown_pct = max_drawdown_pct

        self.current_drawdown = 0
        self.peak_equity = 100000

    def validate_order(self, order: Order, current_portfolio: dict) -> tuple[bool, str]:
        """
        Emir risk kontrolÃ¼nden geÃ§er mi?
        """
        # 1. Position size check
        order_value = order.quantity * order.price
        portfolio_value = current_portfolio['equity']

        if order_value > portfolio_value * self.max_position_size_pct:
            return False, f"Position size exceeds {self.max_position_size_pct*100}% limit"

        # 2. Leverage check
        total_exposure = sum(pos['value'] for pos in current_portfolio['positions'].values())
        if total_exposure + order_value > portfolio_value * self.max_leverage:
            return False, f"Leverage would exceed {self.max_leverage}x"

        # 3. Drawdown circuit breaker
        if self.current_drawdown >= self.max_drawdown_pct:
            return False, f"Circuit breaker: Drawdown {self.current_drawdown*100:.1f}% >= {self.max_drawdown_pct*100}%"

        # 4. Correlation check (aynÄ± yÃ¶nde Ã§ok fazla pozisyon aÃ§ma)
        correlated_positions = self.get_correlated_positions(order.symbol, current_portfolio)
        if len(correlated_positions) >= 5:
            return False, "Too many correlated positions"

        return True, "OK"

    def update_drawdown(self, current_equity):
        """Drawdown gÃ¼ncelle"""
        if current_equity > self.peak_equity:
            self.peak_equity = current_equity

        self.current_drawdown = (self.peak_equity - current_equity) / self.peak_equity
```

---

## ğŸ¯ SONUÃ‡ VE Ã–NERÄ°LER

### âœ… Projenin GÃ¼Ã§lÃ¼ YÃ¶nleri:
1. **Hibrit ML yaklaÅŸÄ±mÄ±** (Supervised + RL) - en gÃ¼ncel ve etkili
2. **Multi-modal veri** (teknik, on-chain, tÃ¼rev, psikoloji)
3. **GerÃ§ekÃ§i backtest** (slippage, komisyon, market impact)
4. **SÃ¼rekli Ã¶ÄŸrenme** (PDF RAG + RL fine-tuning)
5. **Professional UI** (React + real-time)

### âš ï¸ Riskler ve Zorluklar:
1. **Kompleksite** - Ã§ok katmanlÄ± sistem, debug zor olabilir
2. **Veri kalitesi** - farklÄ± kaynaklardan gelen data tutarsÄ±zlÄ±klarÄ±
3. **Overfitting** - Ã¶zellikle RL'de, gerÃ§ek piyasada performans dÃ¼ÅŸebilir
4. **Latency** - real-time karar vermede gecikme kritik

### ğŸš€ Ä°yileÅŸtirme Ã–nerileri:
1. **A/B Testing** - farklÄ± RL stratejilerini paralel test et
2. **Ensemble of Ensembles** - birden fazla RL agent'Ä± bir araya getir
3. **Adversarial Training** - kriz senaryolarÄ±na karÅŸÄ± robust yap
4. **Human-in-the-loop** - kritik kararlarda insan onayÄ±

### ğŸ“Š BaÅŸarÄ± Metrikleri (6 Ay SonrasÄ± Hedef):
- Sharpe Ratio: > 2.0
- Win Rate: > 55%
- Max Drawdown: < 15%
- Backtest-to-Live Performance Gap: < 10%

---

## ğŸ“š Ek Kaynaklar

1. **RL for Trading Papers:**
   - "Deep Reinforcement Learning for Trading" (2019)
   - "Decision Transformer: Reinforcement Learning via Sequence Modeling" (2021)
   - "Generative Adversarial Imitation Learning" (2016)

2. **Crypto-specific:**
   - "Machine Learning for Cryptocurrency Trading" (2020)
   - "On-Chain Metrics for Bitcoin Price Prediction" (2021)

3. **Implementation Guides:**
   - Stable-Baselines3 documentation
   - RLlib documentation
   - FinRL library

---

**SON NOT:**
Bu mimari, akademik dÃ¼zeyde bir sistem iÃ§in saÄŸlam bir temel oluÅŸturuyor. Production'a geÃ§meden Ã¶nce mutlaka:
1. Extensive backtesting (en az 3 yÄ±l)
2. Paper trading (en az 3 ay)
3. Small capital live testing (en az 1 ay)
yapÄ±lmalÄ±.

**Risk UyarÄ±sÄ±:** Kripto piyasalarÄ± son derece volatil. HiÃ§bir AI sistemi %100 doÄŸru tahmin yapamaz. Her zaman stop-loss kullanÄ±n ve kaybedebileceÄŸinizden fazlasÄ±nÄ± riske atmayÄ±n.
